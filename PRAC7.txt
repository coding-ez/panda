

    1)
import nltk
from nltk import word_tokenize, sent_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
    
    2)
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

3)
text = "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fusce commodo mauris id justo condimentum dignissim. Nullam placerat semper dapibus. Pellentesque ac risus nulla. Phasellus ut dapibus nunc, id aliquam dolor."

4)print(word_tokenize(text))
5)print(sent_tokenize(text))
6)to_tag = word_tokenize(text)

7)print(pos_tag(to_tag))

8) 
stop_words = set(stopwords.words("english"))
print(stop_words)

9)
to_clean = word_tokenize(text)
to_clean
10)
no_stopwords_text = []
    for token in to_clean:
        if(token not in stop_words):
            no_stopwords_text.append(token)

print(no_stopwords_text)

11)stemmer = PorterStemmer()

12)stemmed_words = []
for token in no_stopwords_text:
	stemmed_word = stemmer.stem(token)
	stemmed_words.append(stemmed_word)

13)print(stemmed_words)
    
14)lemmatizer = WordNetLemmatizer()

15)lemmatized_words = []
for token in no_stopwords_text:
	lemmatized = lemmatizer.lemmatize(token)
        lemmatized_words.append(lemmatized)

    16)print(lemmatized_words)
    17)vectorizer = TfidfVectorizer()

18)corpus = [
        "I love to eat pizza",
        "Pizza is my favorite food",
        "I enjoy eating pizza with friends",
        "I like to have pizza for dinner",
        "Pizza toppings include cheese, pepperoni, and mushrooms"]

19)vectorizer = TfidfVectorizer()
vectorizer

20)tfidf_matrix = vectorizer.fit_transform(corpus)
feature_names = vectorizer.get_feature_names_out()


21)print(tfidf_matrix.toarray())
print(feature_names)
